
# JurisRAG ‚öñ

# Legal Query Assistant powered by Retrieval_Augmented Generation (RAG)

**Deployed**:[]

This project implements a Retrieval-Augmented Generation pipeline for answering domain-specific questions (legal in this case). It uses LangChain to orchestrate LLMs, Pinecone as a vector database for semantic search, and Cohere Reranker to improve retrieval quality. The backend is served using FastAPI and can be deployed on Render or any cloud platform.

## üöÄFeatures
Document Ingestion & Chunking  ‚Äì Splits PDFs / text into semantic chunks.

Vector Storage ‚Äì Stores embeddings in Pinecone for scalable retrieval.

Retriever ‚Äì Uses MMR (Maximal Marginal Relevance) for diverse, relevant results.

Reranker ‚Äì Cohere Rerank API improves retrieved context before LLM answers.

LLM-powered Responses ‚Äì OpenAI GPT (or other LLMs) generates contextual answers.

API Endpoint ‚Äì /chat for query ‚Üí response interaction.

-Deployment-ready ‚Äì Easily deployable on Render or similar services.


## Tech Stack

**Backend:** Python,FastAPI ,Uvicorn

**Frontend:** HTML ,JavaScript

**AI:** OpenAI LLm, Langchain

**Vector DB:** Pinecone Serverless

**Reranker:** Cohere Reranker API

**Deployment:** Render ,Docker

## Chunking Parameters

üìùText/document processing is done using semantic chunking:

Chunk size: 800 tokens

Chunk overlap: 80 tokens

Purpose: Ensure context continuity for retrieval
## üîç Retriever & Reranker Settings
 

**Retriever (MMR)**

Type: Maximal Marginal Relevance

k: 5 (number of top documents retrieved)

lambda_mult: 0.5 (balance between relevance & diversity)


**Reranker (Cohere)**

API: rerank endpoint

Top k rerank: 3

Input: Retrieved document chunks

Output: Ranked list of chunks fed to LLM
## ARCHITECTURE

![image alt](https://github.com/ankuu63/JurisRag_pinecone_fresh/blob/028df29bf6f7c921619df6cf0cc95921dabb5bc0/dia.drawio.png)


## Quick Start

1. Clone the repository


```bash
  git clone https://github.com/ankuu63/JurisRag_pinecone_fresh.git
  cd Juris_RAG_APP-
```

2. Create and activate a virtual environment 


```bash
  python -m venv venv
  source venv/bin/activate  # Linux/Mac
  venv\Scripts\activate     # Windows
```

3. Install Dependencies


```bash
  pip install -r requirements.txt
```

4. Set environmental variables


```bash
  export OPENAI_API_KEY="your_openai_key"  # Linux/Mac
  set OPENAI_API_KEY="your_openai_key"     # Windows
      PINECONE_API_KEY="your_pinecone_key"
      COHERE_API_KEY="your_cohere_key"
```

5. Run locally


```
  uvicorn rag.app:app --reload
```


Open http://localhost:8000 to view the app.



## Usage


1. Upload PDF via the web interface.


2. Enter your legal query in the input field.


3. Click Submit.


4. Retrieve answer generated by AI based on uploaded document.
## Deployment

üåê Deployment (Render)

1. Push code to GitHub ( repo).


2. Connect repo to Render.


3. Add environment variables in Render dashboard.


4. Deploy service ‚Üí Get live API endpoint.



